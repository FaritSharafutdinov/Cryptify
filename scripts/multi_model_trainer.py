# –§–∞–π–ª: multi_model_trainer.py

import json
import pandas as pd
import numpy as np
import sys
from sqlalchemy import create_engine, text
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import joblib
import json
from tensorflow.keras.metrics import MeanSquaredError

# 3rd Party Models
import xgboost as xgb
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.metrics import MeanSquaredError # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç –¥–ª—è load_model

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø DB ---
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –æ–±—â–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
try:
    from config import DATABASE_URL, DB_TABLE_FEATURES, TARGET_HORIZONS
    DB_TABLE_FEATURES = DB_TABLE_FEATURES
    TARGET_HORIZONS = TARGET_HORIZONS
except ImportError:
    # Fallback –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    import os
    DB_USER = os.getenv("DB_USER", "criptify_user")
    DB_PASSWORD = os.getenv("DB_PASSWORD", "criptify_password")
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_NAME = os.getenv("DB_NAME", "criptify_db")
    DB_PORT = os.getenv("DB_PORT", "5432")
    DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    DB_TABLE_FEATURES = "btc_features_1h"
    TARGET_HORIZONS = [6, 12, 24]

ENGINE = create_engine(DATABASE_URL)

# --- –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ï–ô ---
TARGET_HORIZONS = [6, 12, 24] # –ü—Ä–æ–≥–Ω–æ–∑ Log Return –Ω–∞ 6, 12 –∏ 24 —á–∞—Å–∞
LSTM_WINDOW_SIZE = 48 # –†–∞–∑–º–µ—Ä —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –¥–ª—è LSTM
RETRAIN_PERIOD_DAYS = 90 # –î–æ–æ–±—É—á–∞–µ–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π
METRICS_FILENAME = "prediction_metrics.json"

# –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ data_fetcher
BASE_FEATURES = [
    'log_return', 'SP500_log_return', 'price_range', 'price_change',
    'volatility_5', 'volatility_14', 'volume_ma_5', 'volume_zscore',
    'MACD_safe', 'RSI_safe', 'ATR_safe_norm', 'hour_sin', 'hour_cos'
]
MODEL_ERRORS = {}
# --- –§–£–ù–ö–¶–ò–ò –ë–ê–ó–´ –î–ê–ù–ù–´–• ---

def load_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã btc_features_1h."""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã features...")

    sql_query = f"SELECT * FROM {DB_TABLE_FEATURES} ORDER BY timestamp ASC;"

    try:
        df = pd.read_sql(
            sql_query, ENGINE, index_col="timestamp", parse_dates=["timestamp"]
        )
        # ‚ö†Ô∏è –í–ê–ñ–ù–û: –£–î–ê–õ–Ø–ï–ú –ö–û–õ–û–ù–ö–ò OPEN_INTEREST –ò SP500, –ö–û–¢–û–†–´–ï –ù–ï –§–ò–ß–ò,
        # –ï–°–õ–ò –û–ù–ò –ë–´–õ–ò –°–û–•–†–ê–ù–ï–ù–´.
        # –í –≤–∞—à–µ–º data_collector.py —Ñ–∏—á–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è, –ø–æ—ç—Ç–æ–º—É BASE_FEATURES
        # –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω.
        df = df.dropna()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫.")
        return df
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return pd.DataFrame()

def save_model_metrics(model_name: str, target: str, metrics: dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É ml_models, –∏—Å–ø–æ–ª—å–∑—É—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á (model_name, target)."""
    full_model_name = f"{model_name}_{target}"
    
    sql_query = text(
        """
        INSERT INTO ml_models (model_name, metrics, updated_at)
        VALUES (:model_name_param, CAST(:metrics_param AS jsonb), NOW())
        ON CONFLICT (model_name) DO UPDATE
        SET metrics = CAST(:metrics_param AS jsonb), updated_at = NOW()
        """
    )
    
    try:
        metrics_payload = json.dumps(metrics)
        with ENGINE.begin() as connection:
            connection.execute(
                sql_query,
                {
                    "metrics_param": metrics_payload, 
                    "model_name_param": full_model_name
                },
            )
            print(f"–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ {full_model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã/–æ–±–Ω–æ–≤–ª–µ–Ω—ã.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {e}")
        print(f"SQL State: {e.orig.pgcode if hasattr(e.orig, 'pgcode') else 'N/A'}")
        
def ensure_table_exists():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É ml_models, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã ml_models...")
    
    create_table_sql = text("""
        CREATE TABLE IF NOT EXISTS ml_models (
            model_name VARCHAR(255) PRIMARY KEY,
            metrics JSONB,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        );
    """)
    
    try:
        with ENGINE.begin() as connection:
            connection.execute(create_table_sql)
        print("–¢–∞–±–ª–∏—Ü–∞ ml_models –≥–æ—Ç–æ–≤–∞.")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã ml_models: {e}")
        sys.exit(1)

# --- –§–£–ù–ö–¶–ò–Ø –°–û–ó–î–ê–ù–ò–Ø –¢–ê–†–ì–ï–¢–û–í ---

def create_targets(df: pd.DataFrame):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ç—Ä–∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (Y): Log Return –Ω–∞ 6h, 12h, 24h.
    """
    df_temp = df.copy()
    
    # ‚ö†Ô∏è –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º 'BTC_Close' –∏–ª–∏ 'Close' –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫
    # –≤—ã –Ω–∞–∑–≤–∞–ª–∏ –∫–æ–ª–æ–Ω–∫—É –ø–æ—Å–ª–µ Feature Engineering. –°—É–¥—è –ø–æ –≤–∞—à–µ–º—É
    # data_collector.py, –ø–æ—Å–ª–µ final_df.drop 'BTC_Close' –æ—Å—Ç–∞–Ω–µ—Ç—Å—è, –µ—Å–ª–∏ –≤—ã 
    # –Ω–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏ –µ–≥–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ 'Close'. –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:
    if 'BTC_Close' in df_temp.columns:
        close_prices = df_temp['BTC_Close']
    elif 'Close' in df_temp.columns:
        close_prices = df_temp['Close']
    else:
        print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'Close' –∏–ª–∏ 'BTC_Close' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        sys.exit(1)
    
    for h in TARGET_HORIZONS:
        future_close = close_prices.shift(-h)
        target_col = f"log_return_{h}h"
        # Log Return: ln(Future_Close / Current_Close)
        df_temp[target_col] = np.log(future_close / close_prices)
        print(f"–°–æ–∑–¥–∞–Ω —Ç–∞—Ä–≥–µ—Ç: {target_col}")

    df_temp.dropna(inplace=True)
    
    X = df_temp[BASE_FEATURES].copy()
    Y = df_temp[[f"log_return_{h}h" for h in TARGET_HORIZONS]].copy()

    return X, Y


# --- –ü–†–ï–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ú–û–î–ï–õ–ï–ô ---

def preprocess_linear_xgb(X: pd.DataFrame, Y: pd.DataFrame, test_size=0.2):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è LR –∏ XGBoost: –†–∞–∑–±–∏–µ–Ω–∏–µ, –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è X –∏ 
    –≤–æ–∑–≤—Ä–∞—Ç —Å–∫–µ–π–ª–µ—Ä–∞.
    """
    
    if test_size == 0.0:
        print(" ¬† ¬†[Info] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (test_size=0.0).")
        X_train, X_test = X, pd.DataFrame()
        Y_train, Y_test = Y, pd.DataFrame()
    else:
        X_train, X_test, Y_train, Y_test = train_test_split(
            X, Y, test_size=test_size, shuffle=False
        )
    
    # 1. StandardScaler –¥–ª—è LR
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    if not X_test.empty:
        X_test_scaled = scaler.transform(X_test)
    else:
        X_test_scaled = np.array([])
    
    return (
        X_train_scaled, X_test_scaled, X_train.values, X_test.values, 
        Y_train, Y_test, scaler # ‚ö†Ô∏è –î–û–ë–ê–í–õ–ï–ù –í–û–ó–í–†–ê–¢ –°–ö–ï–ô–õ–ï–†–ê
    )

def create_sliding_window(data, window_size):
    """–°–æ–∑–¥–∞–µ—Ç —Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö LSTM."""
    X_windowed, Y_windowed = [], []
    for i in range(len(data) - window_size):
        # –û–∫–Ω–æ X (48 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤)
        X_windowed.append(data[i:(i + window_size), :])
        # –¶–µ–ª–µ–≤–æ–µ Y (–∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ –æ–∫–Ω–∞)
        Y_windowed.append(data[i + window_size, :])
    return np.array(X_windowed), np.array(Y_windowed)

def save_metrics(metrics):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ RSE/RMSE –≤ JSON-—Ñ–∞–π–ª."""
    with open(METRICS_FILENAME, 'w') as f:
        json.dump(metrics, f, indent=4)
    print(f"  -> –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –æ—à–∏–±–æ–∫ –≤ {METRICS_FILENAME}.")
    
    
def preprocess_lstm(X: pd.DataFrame, Y: pd.DataFrame, test_size=0.2, window_size=LSTM_WINDOW_SIZE):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è LSTM: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è X, Y, –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ 
    –∏ –≤–æ–∑–≤—Ä–∞—Ç —Å–∫–µ–π–ª–µ—Ä–æ–≤.
    """
    
    # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è X –∏ Y
    scaler_x = MinMaxScaler()
    X_scaled = scaler_x.fit_transform(X)
    scaler_y = MinMaxScaler()
    Y_scaled = scaler_y.fit_transform(Y) # Y –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    
    # 2. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ X –∏ Y –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–Ω–∞
    combined_scaled = np.hstack((X_scaled, Y_scaled))

    # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞
    # Y_temp —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ X –∏ Y –≤ –∫–æ–Ω—Ü–µ –æ–∫–Ω–∞
    X_windowed, Y_temp = create_sliding_window(combined_scaled, window_size)
    
    # –û—Ç–¥–µ–ª—è–µ–º —Ç–∞—Ä–≥–µ—Ç—ã Y (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N –∫–æ–ª–æ–Ω–æ–∫ Y_temp)
    Y_windowed = Y_temp[:, -len(TARGET_HORIZONS):]
    
    # –û—Ç–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ X (–ø–µ—Ä–≤—ã–µ N –∫–æ–ª–æ–Ω–æ–∫)
    X_windowed = X_windowed[:, :, :len(BASE_FEATURES)]

    # 4. –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    test_split_index = int(len(X_windowed) * (1 - test_size))
    
    X_train = X_windowed[:test_split_index]
    X_test = X_windowed[test_split_index:]
    Y_train = Y_windowed[:test_split_index]
    Y_test = Y_windowed[test_split_index:]
    
    return X_train, X_test, Y_train, Y_test, scaler_y, scaler_x # ‚ö†Ô∏è –î–û–ë–ê–í–õ–ï–ù –í–û–ó–í–†–ê–¢ –°–ö–ï–ô–õ–ï–†–ê X


# --- –§–£–ù–ö–¶–ò–ò –û–ë–£–ß–ï–ù–ò–Ø –ò –û–¶–ï–ù–ö–ò ---
# (–û—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)

def train_and_evaluate_lr(X_train, X_test, Y_train, Y_test):
    # ... (–û—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    model_name = "LinearRegression"
    for i, h in enumerate(TARGET_HORIZONS):
        target_name = f"log_return_{h}h"
        model = LinearRegression()
        model.fit(X_train, Y_train.iloc[:, i]) 
        
        if len(X_test) > 0:
            predictions = model.predict(X_test)
            mae = mean_absolute_error(Y_test.iloc[:, i], predictions)
            mse = mean_squared_error(Y_test.iloc[:, i], predictions)
            rmse = np.sqrt(mse)
            metrics = {"mae": float(mae), "mse": float(mse)}
            print(f"  -> {model_name} | MAE: {mae:.6f} | RMSE: {rmse:.6f}")
            save_model_metrics(model_name, target_name, metrics)
            MODEL_ERRORS[model_name] = rmse
        joblib.dump(model, f"{model_name}_{target_name}.joblib")

def train_and_evaluate_xgb(X_train, X_test, Y_train, Y_test):
    # ... (–û—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    model_name = "XGBoost"
    for i, h in enumerate(TARGET_HORIZONS):
        target_name = f"log_return_{h}h"
        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
        model.fit(X_train, Y_train.iloc[:, i])
        
        if len(X_test) > 0:
            predictions = model.predict(X_test)
            mae = mean_absolute_error(Y_test.iloc[:, i], predictions)
            mse = mean_squared_error(Y_test.iloc[:, i], predictions)
            rmse = np.sqrt(mse)
            print(f"  -> {model_name} | MAE: {mae:.6f} | RMSE: {rmse:.6f}")
            MODEL_ERRORS[model_name] = rmse
            metrics = {"mae": float(mae), "mse": float(mse)}
            print(f" ¬† ¬† MAE: {mae:.6f}, MSE: {mse:.6f}")
            save_model_metrics(model_name, target_name, metrics)
            
        joblib.dump(model, f"{model_name}_{target_name}.joblib")

def train_and_evaluate_lstm(X_train, X_test, Y_train, Y_test, scaler_y, scaler_x): # ‚ö†Ô∏è –î–û–ë–ê–í–õ–ï–ù –°–ö–ï–ô–õ–ï–† X
    """–û–±—É—á–∞–µ—Ç –æ–¥–Ω—É LSTM –¥–ª—è –≤—Å–µ—Ö 3 —Ç–∞—Ä–≥–µ—Ç–æ–≤."""
    print("\n\n--- –û–±—É—á–µ–Ω–∏–µ LSTM ---")
    
    model_name = "LSTM"
    target_names = [f"log_return_{h}h" for h in TARGET_HORIZONS]
    
    # 1. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM
    model = Sequential([
        LSTM(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),
        Dropout(0.2),
        Dense(len(TARGET_HORIZONS))
    ])
    model.compile(optimizer='adam', loss='mse')
    
    callbacks = []
    if len(X_test) > 0:
        es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        callbacks.append(es)
        
    print(f" ¬†-> –û–±—É—á–µ–Ω–∏–µ LSTM (–æ–∫–Ω–æ {X_train.shape[1]})...")
    
    # 2. –û–±—É—á–µ–Ω–∏–µ
    model.fit(
        X_train, Y_train,
        epochs=50, 
        batch_size=32,
        validation_data=(X_test, Y_test) if len(X_test) > 0 else None,
        callbacks=callbacks,
        verbose=0
    )
    
    # 3. –û—Ü–µ–Ω–∫–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä)
    if len(X_test) > 0:
        predictions_scaled = model.predict(X_test)
        
        Y_test_denorm = scaler_y.inverse_transform(Y_test)
        predictions_denorm = scaler_y.inverse_transform(predictions_scaled)
        
        for i, target_name in enumerate(target_names):
            mae = mean_absolute_error(Y_test_denorm[:, i], predictions_denorm[:, i])
            mse = mean_squared_error(Y_test_denorm[:, i], predictions_denorm[:, i])
            rmse = np.sqrt(mse)
            metrics = {"mae": float(mae), "mse": float(mse)}
            print(f" ¬†-> {target_name} | MAE: {mae:.6f}, MSE: {mse:.6f}")
            save_model_metrics(model_name, target_name, metrics)
            MODEL_ERRORS[model_name] = rmse
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    model.save(f"{model_name}.h5", save_format='tf')
    
    # ‚ö†Ô∏è –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ X –∏ Y –¥–ª—è LSTM
    joblib.dump(scaler_x, f"{model_name}_X_scaler.joblib")
    print(f"  -> –°–æ—Ö—Ä–∞–Ω–µ–Ω {model_name}_X_scaler.joblib.")
    
    # ‚ö†Ô∏è –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–æ–≤ Y (target) –¥–ª—è LSTM 
    # (–•–æ—Ç—è predictor.py –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–≥–ª—É—à–∫–∏, —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã)
    for h in TARGET_HORIZONS:
        # NOTE: –ü–æ—Å–∫–æ–ª—å–∫—É LSTM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–∏–Ω MinMaxScaler –¥–ª—è –≤—Å–µ—Ö Y, 
        # –¥–ª—è LR/XGB –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∫–µ–π–ª–µ—Ä—ã Y.
        joblib.dump(scaler_y, f"{model_name}_Y_scaler_{h}h.joblib")
        
    print("==================================================================")
    print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–µ–π (RMSE).")
    try:
        with open("model_errors.json", "w") as f:
            json.dump(MODEL_ERRORS, f, indent=4)
        print("  -> –û—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ model_errors.json")
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è model_errors.json: {e}")


# --- –õ–û–ì–ò–ö–ê –î–û–û–ë–£–ß–ï–ù–ò–Ø (RETRAIN) ---

def get_retrain_data(X_base, Y_base, days_to_fetch):
    """–û–±—Ä–µ–∑–∞–µ—Ç X –∏ Y –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N –¥–Ω—è—Ö."""
    end_date = X_base.index.max()
    start_date_retrain = end_date - pd.Timedelta(days=days_to_fetch)
    
    X_retrain = X_base[X_base.index >= start_date_retrain]
    Y_retrain = Y_base[Y_base.index >= start_date_retrain]
    
    print(f"‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å {start_date_retrain.date()} ({len(X_retrain)} —Å—Ç—Ä–æ–∫).")
    
    return X_retrain, Y_retrain

def retrain_all_models(X_base, Y_base):
    """–û—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö RETRAIN_PERIOD_DAYS."""
    print(f"\n\n======================================================================")
    print(f"üîÑ –°–¢–ê–†–¢ –î–û–û–ë–£–ß–ï–ù–ò–Ø (RETRAIN) –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {RETRAIN_PERIOD_DAYS} –¥–Ω—è—Ö")
    print(f"======================================================================")
    
    X_retrain, Y_retrain = get_retrain_data(X_base, Y_base, RETRAIN_PERIOD_DAYS)
    
    # 1. LR –∏ XGBoost (–ü–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–µ–∂–µ–º –Ω–∞–±–æ—Ä–µ)
    Y_train_full = Y_retrain
    
    print("\n--- –î–æ–æ–±—É—á–µ–Ω–∏–µ LR/XGB (–ü–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö) ---")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è LR)
    # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–∑–≤—Ä–∞—â–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä (–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç)
    X_lr_scaled, _, X_xgb_raw, _, Y_train_df, _, scaler_x_lr = preprocess_linear_xgb(X_retrain, Y_retrain, test_size=0.0)
    
    # ‚ö†Ô∏è –ù–û–í–û–ï: –°–û–•–†–ê–ù–ï–ù–ò–ï –°–ö–ï–ô–õ–ï–†–ê X –î–õ–Ø LR/XGB
    joblib.dump(scaler_x_lr, "LR_X_scaler.joblib")
    print("  -> –°–æ—Ö—Ä–∞–Ω–µ–Ω LR_X_scaler.joblib.")
    
    for i, h in enumerate(TARGET_HORIZONS):
        target_name = f"log_return_{h}h"
        
        # LR
        lr_model = LinearRegression()
        lr_model.fit(X_lr_scaled, Y_train_full.iloc[:, i])
        joblib.dump(lr_model, f"LinearRegression_{target_name}.joblib")
        
        # XGBoost
        xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
        xgb_model.fit(X_xgb_raw, Y_train_full.iloc[:, i])
        joblib.dump(xgb_model, f"XGBoost_{target_name}.joblib")
        
        print(f" ¬†-> {target_name} –æ–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è LR –∏ XGBoost.")

    # 2. LSTM (Fine-tuning)
    print("\n--- –î–æ–æ–±—É—á–µ–Ω–∏–µ LSTM (Fine-tuning) ---")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ LSTM –¥–∞–Ω–Ω—ã—Ö
    # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–∑–≤—Ä–∞—â–µ–Ω–Ω—ã–π —Å–∫–µ–π–ª–µ—Ä X –∏ Y –¥–ª—è LSTM
    X_lstm_full, _, Y_lstm_full, _, scaler_y_lstm, scaler_x_lstm = preprocess_lstm(X_retrain, Y_retrain, test_size=0.0)
    
    # ‚ö†Ô∏è –ù–û–í–û–ï: –°–û–•–†–ê–ù–ï–ù–ò–ï –°–ö–ï–ô–õ–ï–†–û–í X –ò Y –î–õ–Ø LSTM
    joblib.dump(scaler_x_lstm, "LSTM_X_scaler.joblib")
    print("  -> –°–æ—Ö—Ä–∞–Ω–µ–Ω LSTM_X_scaler.joblib.")
    for h in TARGET_HORIZONS:
        joblib.dump(scaler_y_lstm, f"LSTM_Y_scaler_{h}h.joblib")

    try:
        # ‚ö†Ô∏è –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–æ MeanSquaredError –≤ custom_objects.
        lstm_model = load_model(
            "LSTM.h5", 
            custom_objects={
                'loss': 'mse',
                'MeanSquaredError': MeanSquaredError
            }
        )
        
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —ç–ø–æ—Ö
        print(" ¬†-> –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π LSTM –º–æ–¥–µ–ª–∏...")
        lstm_model.fit(
            X_lstm_full, Y_lstm_full, 
            epochs=5,
            batch_size=32, 
            verbose=0
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        lstm_model.save("LSTM.h5")
        print(f" ¬†-> LSTM –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ LSTM.h5.")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ LSTM. –í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å LSTM.h5 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –û–±—É—á–∏—Ç–µ –µ—ë —Å–Ω–∞—á–∞–ª–∞ –≤ —Ä–µ–∂–∏–º–µ 'batch'. –û—à–∏–±–∫–∞: {e}")


# --- –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---

if __name__ == "__main__":
    
    mode = 'batch'
    if len(sys.argv) > 1 and sys.argv[1] == 'retrain':
        mode = 'retrain'

    print(f"\n======================================================================")
    print(f"üöÄ –¢–†–ï–ù–ï–† –ú–û–î–ï–õ–ï–ô: –†–ï–ñ–ò–ú - {mode.upper()}")
    print(f"======================================================================")

    ensure_table_exists()
    
    data = load_data()
    if data.empty:
        sys.exit(1)
        
    X_base, Y_base = create_targets(data)
    
    if mode == 'batch':
        # --- –ü–û–õ–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï (BATCH TRAINING) ---

        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è LR –∏ XGBoost
        # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä X –¥–ª—è LR/XGB
        (
            X_lr_train, X_lr_test, 
            X_xgb_train, X_xgb_test, 
            Y_train_df, Y_test_df, 
            scaler_x_lr # ‚ö†Ô∏è –ù–û–í–´–ô –í–û–ó–í–†–ê–©–ê–ï–ú–´–ô –ü–ê–†–ê–ú–ï–¢–†
        ) = preprocess_linear_xgb(X_base, Y_base)
        
        # ‚ö†Ô∏è –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∫–µ–π–ª–µ—Ä–∞ X –¥–ª—è LR/XGB
        joblib.dump(scaler_x_lr, "LR_X_scaler.joblib")
        print("  -> –°–æ—Ö—Ä–∞–Ω–µ–Ω LR_X_scaler.joblib.")
        
        # 2. –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ LR
        train_and_evaluate_lr(
            pd.DataFrame(X_lr_train, columns=X_base.columns), 
            pd.DataFrame(X_lr_test, columns=X_base.columns), 
            Y_train_df, Y_test_df
        )
        
        # 3. –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ XGBoost
        train_and_evaluate_xgb(
            pd.DataFrame(X_xgb_train, columns=X_base.columns), 
            pd.DataFrame(X_xgb_test, columns=X_base.columns), 
            Y_train_df, Y_test_df
        )
        
        # 4. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞, –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ LSTM
        # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä X –¥–ª—è LSTM
        X_lstm_train, X_lstm_test, Y_lstm_train, Y_lstm_test, scaler_y_lstm, scaler_x_lstm = preprocess_lstm(X_base, Y_base)
        train_and_evaluate_lstm(X_lstm_train, X_lstm_test, Y_lstm_train, Y_lstm_test, scaler_y_lstm, scaler_x_lstm)

    elif mode == 'retrain':
        # --- –î–û–û–ë–£–ß–ï–ù–ò–ï (RETRAIN MODE) ---
        retrain_all_models(X_base, Y_base)
        
    print("\n\n‚úÖ –û–±—É—á–µ–Ω–∏–µ/–î–æ–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")