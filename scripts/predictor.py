# predictor.py

import os
import sys
import joblib
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import json
from sqlalchemy import create_engine, text
from sklearn.preprocessing import StandardScaler
from scipy.stats import norm

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è TensorFlow (–¥–ª—è —á–∏—Å—Ç–æ—Ç—ã)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
try:
    from tensorflow.keras.models import load_model
    from tensorflow.keras.metrics import MeanSquaredError
except ImportError:
    print("‚ùå –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –§—É–Ω–∫—Ü–∏–∏ LSTM –±—É–¥—É—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
    load_model = None
    MeanSquaredError = None

# --- –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò ---
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –æ–±—â–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
try:
    from config import (
        DATABASE_URL_SQLALCHEMY, DB_TABLE_FEATURES, TARGET_HORIZONS
    )
    DB_URL = DATABASE_URL_SQLALCHEMY
    DB_TABLE_FEATURES = DB_TABLE_FEATURES
    TARGET_HORIZONS = TARGET_HORIZONS
except ImportError:
    # Fallback –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_USER = os.getenv("DB_USER", "criptify_user")
    DB_PASSWORD = os.getenv("DB_PASSWORD", "criptify_password")
    DB_NAME = os.getenv("DB_NAME", "criptify_db")
    DB_URL = f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:5432/{DB_NAME}"
    DB_TABLE_FEATURES = "btc_features_1h"
    TARGET_HORIZONS = [6, 12, 24]

ENGINE = create_engine(DB_URL)

MODEL_DIR = "."  # –ú–æ–¥–µ–ª–∏ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
TARGET_HORIZONS = [6, 12, 24] # –ß–∞—Å—ã
Z_SCORE_95 = 1.96
MODEL_ERRORS = {}

# ‚ö†Ô∏è –ò–ú–Ø –¢–ê–ë–õ–ò–¶–´ –§–ò–ß–ï–ô –î–û–õ–ñ–ù–û –°–û–û–¢–í–ï–¢–°–¢–í–û–í–ê–¢–¨ data_collector.py
# DB_TABLE_FEATURES —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤—ã—à–µ
LSTM_TIME_STEPS = 48 # –û–∫–Ω–æ –¥–ª—è LSTM

# üîë –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–ü–ò–°–û–ö –ü–†–ò–ó–ù–ê–ö–û–í –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
BASE_FEATURES = [
    'log_return', 'SP500_log_return', 'price_range', 'price_change',
    'volatility_5', 'volatility_14', 'volume_ma_5', 'volume_zscore',
    'MACD_safe', 'RSI_safe', 'ATR_safe_norm', 'hour_sin', 'hour_cos'
]
# ----------------------------------------------------------------------

# --- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (–¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ Y) ---
# ‚ö†Ô∏è –í–ê–ñ–ù–û: –ú—ã —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É, —Ç–∞–∫ –∫–∞–∫ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å/–∑–∞–≥—Ä—É–∂–∞—Ç—å
# —Å–∫–µ–π–ª–µ—Ä Y, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
def create_dummy_scaler(mean, scale):
    """–°–æ–∑–¥–∞–µ—Ç –∑–∞–≥–ª—É—à–∫—É —Å–∫–µ–π–ª–µ—Ä–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º mean –∏ scale –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è."""
    scaler = StandardScaler()
    # –ó–∞–¥–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ –º–∞—Å—à—Ç–∞–±, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –±—ã–ª –æ–±—É—á–µ–Ω —Å–∫–µ–π–ª–µ—Ä
    scaler.mean_ = np.array([mean])
    scaler.scale_ = np.array([scale])
    scaler.var_ = np.array([scale**2])
    scaler.n_features_in_ = 1
    return scaler

# –£—Å–ª–æ–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è Log Return (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –æ–±—É—á–µ–Ω–∏—è)
DUMMY_SCALER_PARAMS = {
    6: {'mean': 0.000001, 'scale': 0.005},
    12: {'mean': 0.000002, 'scale': 0.008},
    24: {'mean': 0.000005, 'scale': 0.012},
}
# ----------------------------------------------------------------------

def ensure_prediction_table_exists():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã predictions...")
    
    create_table_sql = text("""
        CREATE TABLE IF NOT EXISTS predictions (
            time TIMESTAMP WITH TIME ZONE NOT NULL,
            model_name VARCHAR(255) NOT NULL,
            target_hours INTEGER NOT NULL,
            prediction_log_return FLOAT, -- –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ª–æ–≥-–¥–æ—Ö–æ–¥
            ci_low FLOAT, -- –ù–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            ci_high FLOAT, -- –í–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
            created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
            PRIMARY KEY (time, model_name, target_hours)
        );
        CREATE INDEX IF NOT EXISTS idx_predictions_time ON predictions (time);
    """)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ ci_low –∏ ci_high –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    alter_table_sql = text("""
        DO $$ 
        BEGIN
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
                          WHERE table_name='predictions' AND column_name='ci_low') THEN
                ALTER TABLE predictions ADD COLUMN ci_low FLOAT;
            END IF;
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
                          WHERE table_name='predictions' AND column_name='ci_high') THEN
                ALTER TABLE predictions ADD COLUMN ci_high FLOAT;
            END IF;
        END $$;
    """)
    
    try:
        with ENGINE.begin() as connection:
            connection.execute(create_table_sql)
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª–∞
            connection.execute(alter_table_sql)
        print("–¢–∞–±–ª–∏—Ü–∞ predictions –≥–æ—Ç–æ–≤–∞.")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã predictions: {e}")
        sys.exit(1)


def load_latest_data(minutes_count: int = 50):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã features –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–Ω–∞ 
    –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {minutes_count} —Å—Ç—Ä–æ–∫ —Ñ–∏—á –∏–∑ DB...")
    
    # ‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å—Ç–æ–ª–±—Ü—ã, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –Ω—É–∂–Ω—ã–µ
    query = f"""
    SELECT *
    FROM {DB_TABLE_FEATURES}
    ORDER BY timestamp DESC
    LIMIT {minutes_count};
    """
    try:
        with ENGINE.connect() as connection:
            df = pd.read_sql(query, connection, index_col='timestamp') 
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
        df = df.sort_index()
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ —Ñ–∏—á. –°–∞–º–∞—è –ø–æ—Å–ª–µ–¥–Ω—è—è: {df.index[-1]}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (—Ç–∞–±–ª–∏—Ü–∞ {DB_TABLE_FEATURES} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç): {e}")
        return pd.DataFrame()

# predictor.py (—Ñ—É–Ω–∫—Ü–∏—è save_prediction)

def load_model_errors():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç RMSE –º–æ–¥–µ–ª–µ–π –∏–∑ model_errors.json."""
    global MODEL_ERRORS
    try:
        with open(os.path.join(MODEL_DIR, "model_errors.json"), "r") as f:
            MODEL_ERRORS = json.load(f)
        print("‚úÖ –û—à–∏–±–∫–∏ –º–æ–¥–µ–ª–µ–π (RMSE) —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
    except FileNotFoundError:
        print("‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª model_errors.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã CI –±—É–¥—É—Ç 0.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ model_errors.json: {e}")

def cleanup_old_predictions(keep_hours: int = 48):
    """
    –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤.
    
    Args:
        keep_hours: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 48 —á–∞—Å–æ–≤ = 2 –¥–Ω—è)
    """
    try:
        from datetime import timezone
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º timezone-aware datetime –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=keep_hours)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
        count_sql = text("""
            SELECT COUNT(*) 
            FROM predictions 
            WHERE time < :cutoff_time
        """)
        
        with ENGINE.connect() as connection:
            count_result = connection.execute(count_sql, {"cutoff_time": cutoff_time})
            count = count_result.scalar()
        
        if count == 0:
            print(f"‚úÖ –°—Ç–∞—Ä—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ {keep_hours} —á–∞—Å–æ–≤)")
            return
        
        print(f"üßπ –ù–∞–π–¥–µ–Ω–æ {count} —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (—Å—Ç–∞—Ä—à–µ {cutoff_time} UTC)")
        
        delete_sql = text("""
            DELETE FROM predictions 
            WHERE time < :cutoff_time
        """)
        
        with ENGINE.begin() as connection:
            result = connection.execute(delete_sql, {"cutoff_time": cutoff_time})
            deleted_count = result.rowcount
        
        if deleted_count > 0:
            print(f"üßπ –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (—Å—Ç–∞—Ä—à–µ {keep_hours} —á–∞—Å–æ–≤)")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å
            remaining_sql = text("SELECT COUNT(*) FROM predictions")
            with ENGINE.connect() as connection:
                remaining = connection.execute(remaining_sql).scalar()
            print(f"üìä –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤ –±–∞–∑–µ: {remaining}")
        else:
            print(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ —É–¥–∞–ª–µ–Ω–æ")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {e}")
        import traceback
        traceback.print_exc()
        
# –§–∞–π–ª: predictor.py

# –ò–ó–ú–ï–ù–ï–ù–ò–ï –°–ò–ì–ù–ê–¢–£–†–´: –¢–µ–ø–µ—Ä—å —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç 6 –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –≤–º–µ—Å—Ç–æ 4
def save_prediction(time: datetime, model_name: str, target_hours: int, prediction: float, ci_low: float, ci_high: float):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ–≥–Ω–æ–∑ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π –¥–æ—Ö–æ–¥) –∏ –µ–≥–æ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
    –≤ —Ç–∞–±–ª–∏—Ü—É predictions, –∏—Å–ø–æ–ª—å–∑—É—è UPSERT.
    """
    
    prediction_val = float(prediction)
    ci_low_val = float(ci_low)
    ci_high_val = float(ci_high) 
    
    # üí° –ò–ó–ú–ï–ù–ï–ù–ò–ï DML: –î–æ–±–∞–≤–ª–µ–Ω—ã ci_low –∏ ci_high –≤ INSERT –∏ UPDATE
    sql_query = text("""
        INSERT INTO predictions (time, model_name, target_hours, prediction_log_return, ci_low, ci_high)
        VALUES (:time, :model_name, :target_hours, :prediction, :ci_low, :ci_high)
        ON CONFLICT (time, model_name, target_hours) DO UPDATE
        SET prediction_log_return = :prediction, 
            ci_low = :ci_low, 
            ci_high = :ci_high, 
            created_at = NOW()
        """
    )
    
    try:
        with ENGINE.begin() as connection:
            connection.execute(
                sql_query,
                {
                    "time": time, 
                    "model_name": model_name,
                    "target_hours": target_hours,
                    "prediction": prediction_val, 
                    "ci_low": ci_low_val, # ‚ö†Ô∏è –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
                    "ci_high": ci_high_val # ‚ö†Ô∏è –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
                }
            )
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ {model_name}/{target_hours}h: {e}")


def load_model_and_predict(model_path: str, model_type: str, X_latest: pd.DataFrame, target_h: int = None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–µ–Ω–æ–º–∞–ª–∏–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–µ–Ω–æ–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–ª–æ–≥-–¥–æ—Ö–æ–¥).
    """
    predictions_scaled = []
    
    # 1. LR –∏ XGBoost
    if model_type in ['LR', 'XGB']:
        try:
            model = joblib.load(model_path)
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_type}_{target_h}h –∏–ª–∏ —Ñ–∞–π–ª —Å–∫–µ–π–ª–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
            return [np.nan]

        # ‚ö†Ô∏è –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ BASE_FEATURES –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        # –î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—É—é –ø–æ –Ω—É–∂–Ω—ã–º —Ñ–∏—á–∞–º
        X_pred_series = X_latest.iloc[-1][BASE_FEATURES] 

        if model_type == 'LR':
            try:
                # LR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ X
                scaler_X = joblib.load(os.path.join(MODEL_DIR, "LR_X_scaler.joblib")) 
                X_pred_scaled = scaler_X.transform(X_pred_series.values.reshape(1, -1))
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏/–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LR_X_scaler: {e}")
                return [np.nan]
        else:
            # XGBoost –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ X
            X_pred_scaled = X_pred_series.values.reshape(1, -1)
            
        preds_scaled = model.predict(X_pred_scaled)
        predictions_scaled = preds_scaled.flatten().tolist()

    # 2. LSTM
    elif model_type == 'LSTM':
        if load_model is None:
            return [np.nan] * len(TARGET_HORIZONS)
        
        try:
            # ‚ö†Ô∏è –§–ò–ù–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï KERAS
            lstm_model = load_model(model_path, compile=False)
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å LSTM –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –æ—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return [np.nan] * len(TARGET_HORIZONS)

        # 1. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 48 —Å—Ç—Ä–æ–∫)
        if len(X_latest) < LSTM_TIME_STEPS:
            print(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(X_latest)} < {LSTM_TIME_STEPS}) –¥–ª—è –æ–∫–Ω–∞ LSTM.")
            return [np.nan] * len(TARGET_HORIZONS)

        # ‚ö†Ô∏è –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ BASE_FEATURES
        X_window = X_latest.iloc[-LSTM_TIME_STEPS:][BASE_FEATURES].values
        
        # 2. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–∫–µ–π–ª–µ—Ä X_LSTM)
        try:
            scaler_X = joblib.load(os.path.join(MODEL_DIR, "LSTM_X_scaler.joblib")) 
            X_scaled = scaler_X.transform(X_window)
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏/–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LSTM_X_scaler: {e}")
            return [np.nan] * len(TARGET_HORIZONS)
            
        # 3. –†–µ—à–µ–π–ø –¥–ª—è LSTM: (1, 48, n_features)
        X_pred = X_scaled.reshape(1, LSTM_TIME_STEPS, X_scaled.shape[1])
        
        # 4. –ü—Ä–æ–≥–Ω–æ–∑
        preds_scaled = lstm_model.predict(X_pred, verbose=0)
        predictions_scaled = preds_scaled.flatten().tolist()
    
    else:
        return [np.nan] * len(TARGET_HORIZONS)
    
    # --- –î–ï–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–û–ì–ù–û–ó–ê ---
    predictions_denorm = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    horizons_to_process = TARGET_HORIZONS if model_type == 'LSTM' else [target_h]

    for i, h in enumerate(horizons_to_process):
        # ‚ö†Ô∏è –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É —Å–∫–µ–π–ª–µ—Ä–∞ –¥–ª—è —ç—Ç–æ–≥–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞
        scaler_y = create_dummy_scaler(**DUMMY_SCALER_PARAMS.get(h, {'mean': 0, 'scale': 1}))
        
        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: scaler.inverse_transform([[scaled_value]])
        if predictions_scaled and i < len(predictions_scaled):
            scaled_val = predictions_scaled[i]
            denorm_val = scaler_y.inverse_transform([[scaled_val]])[0][0]
            predictions_denorm.append(denorm_val)
        else:
            predictions_denorm.append(np.nan)
        
    return predictions_denorm


def run_prediction():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏."""
    
    print("\n=================================================")
    print("‚ú® –°–¢–ê–†–¢ –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø (INFERENCE)")
    print("=================================================")
    
    ensure_prediction_table_exists()
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–æ–≤—ã—Ö
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 48 —á–∞—Å–æ–≤ (2 –¥–Ω—è) –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
    cleanup_old_predictions(keep_hours=48)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –∑–∞–ø–∞—Å–æ–º –Ω–∞ LSTM (48) + 5
    X_latest_df = load_latest_data(minutes_count=LSTM_TIME_STEPS + 5) 
    if X_latest_df.empty:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        return
        
    # –í—Ä–µ–º—è, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –¥–µ–ª–∞–µ—Ç—Å—è –ø—Ä–æ–≥–Ω–æ–∑ (–≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏)
    prediction_time = X_latest_df.index[-1].to_pydatetime()
    print(f"–ü—Ä–æ–≥–Ω–æ–∑ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–∏: {prediction_time}")

    MODELS = {
        'LinearRegression': 'LR',
        'XGBoost': 'XGB',
        'LSTM': 'LSTM'
    }
    
    # ‚ö†Ô∏è –î–û–ë–ê–í–õ–ï–ù–û: –ó–∞–≥—Ä—É–∑–∫–∞ –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ CI
    load_model_errors() 
    
    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for model_name_full, model_type in MODELS.items():
        
        if model_type == 'LSTM':
            # LSTM –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç –≤—Å–µ 3 —Ç–∞—Ä–≥–µ—Ç–∞ —Å—Ä–∞–∑—É
            model_path = os.path.join(MODEL_DIR, "LSTM.h5")
            preds_denorm = load_model_and_predict(model_path, model_type, X_latest_df)
            
            for i, h in enumerate(TARGET_HORIZONS):
                # ‚ö†Ô∏è –ù–û–í–û–ï: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –∏ –ø–æ–ª–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏
                prediction = preds_denorm[i]
                model_name = f"{model_name_full}_log_return_{h}h"
                
                # ‚ö†Ô∏è –ù–û–í–û–ï: –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (CI)
                # –ü–æ–ª—É—á–∞–µ–º RMSE (–∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏, —Ç.–∫. –≤ model_errors.json –∫–ª—é—á–∏ –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞)
                rmse = MODEL_ERRORS.get(model_name_full, 0)
                ci_margin = Z_SCORE_95 * rmse
                ci_low = prediction - ci_margin
                ci_high = prediction + ci_margin
                
                # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï –í–´–ó–û–í–ê: –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º CI –≥—Ä–∞–Ω–∏—Ü—ã
                save_prediction(prediction_time, model_name, h, prediction, ci_low, ci_high)
                
                # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï –í–´–í–û–î–ê: –¢–µ–ø–µ—Ä—å –≤—ã–≤–æ–¥–∏–º CI
                print(f"  -> {model_name_full} {h}h Log Ret: {prediction:.8f} | CI 95%: [{ci_low:.8f}, {ci_high:.8f}]")
                
        else:
            # LR –∏ XGBoost –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç –∫–∞–∂–¥—ã–π —Ç–∞—Ä–≥–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
            for h in TARGET_HORIZONS:
                model_name = f"{model_name_full}_log_return_{h}h"
                model_path = os.path.join(MODEL_DIR, f"{model_name}.joblib")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º
                preds_denorm = load_model_and_predict(model_path, model_type, X_latest_df, target_h=h)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –£ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—Ä–æ–≥–Ω–æ–∑
                prediction_val = preds_denorm[0] if preds_denorm and np.isfinite(preds_denorm[0]) else np.nan
                
                # ‚ö†Ô∏è –ù–û–í–û–ï: –†–∞—Å—á–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ (CI)
                prediction = prediction_val
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ (–±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞), —Ç.–∫. –≤ model_errors.json –∫–ª—é—á–∏ –±–µ–∑ —Å—É—Ñ—Ñ–∏–∫—Å–∞
                base_model_name = model_name_full  # "LinearRegression" –∏–ª–∏ "XGBoost"
                rmse = MODEL_ERRORS.get(base_model_name, 0)
                ci_margin = Z_SCORE_95 * rmse
                ci_low = prediction - ci_margin
                ci_high = prediction + ci_margin
                
                # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï –í–´–ó–û–í–ê: –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–¥–∞–µ–º CI –≥—Ä–∞–Ω–∏—Ü—ã
                save_prediction(prediction_time, model_name, h, prediction_val, ci_low, ci_high)
                
                # ‚ö†Ô∏è –ò–ó–ú–ï–ù–ï–ù–ò–ï –í–´–í–û–î–ê: –¢–µ–ø–µ—Ä—å –≤—ã–≤–æ–¥–∏–º CI
                print(f"  -> {model_name} {h}h Log Ret: {prediction_val:.8f} | CI 95%: [{ci_low:.8f}, {ci_high:.8f}]")

    print("\n‚úÖ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


if __name__ == "__main__":
    run_prediction()